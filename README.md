# CLIP - Multilingual Zero-Shot Classification üåç

Impl√©mentation et am√©lioration du mod√®le CLIP pour la classification zero-shot sur CIFAR-100, avec extension multilingue (Anglais, Fran√ßais, Allemand).

## Structure du projet

```
‚îú‚îÄ‚îÄ Clip.py                              # Zero-shot basique
‚îú‚îÄ‚îÄ Clip_ensembling.py                   # Prompt ensembling  
‚îú‚îÄ‚îÄ multilingual_French_English_CLIP.py  # Am√©lioration: EN + FR
‚îú‚îÄ‚îÄ multilingual_french_german.py        # Am√©lioration: EN + FR + DE
‚îú‚îÄ‚îÄ requirements.txt                     # D√©pendances
‚îî‚îÄ‚îÄ README.md                            
```

## Installation

```bash
# Installer les d√©pendances
pip install -r requirements.txt
```

**Note**: CLIP sera install√© depuis GitHub (inclus dans requirements.txt)

## Ex√©cution

### 1. Reproduction du paper CLIP

```bash
# Zero-shot basique (~63%)
python Clip.py

# Prompt ensembling (~63.72%)
python Clip_ensembling.py
```

### 2. Am√©lioration multilingue

```bash
# Comparaison Anglais vs Fran√ßais
python multilingual_French_English_CLIP.py

# Comparaison 3 langues (Anglais, Fran√ßais, Allemand)
python multilingual_french_german.py
```

## R√©sultats obtenus

### Impl√©mentations du paper

| M√©thode | Accuracy CIFAR-100 |
|---------|-------------------|
| Zero-shot basique | 62.93% |
| Prompt ensembling | 63.72% |

### Am√©lioration multilingue

#### Langues individuelles
- **Anglais** : 62.93% (baseline)
- **Fran√ßais** : 39.80% (√ó40 mieux que le hasard)
- **Allemand** : 37.20%

#### Combinaisons
- **EN+FR** : 57.95%
- **EN+DE** : 57.78%
- **FR+DE** : 42.09%
- **EN+FR+DE** : 55.86%



## Insights principaux

1. **CLIP poss√®de des capacit√©s multilingues implicites**
   - Fonctionne en fran√ßais et allemand sans entra√Ænement explicite
   - Corr√©lations cross-linguales ~0.34

2. **L'anglais reste optimal pour l'usage g√©n√©ral**
   - Meilleure performance globale
   - CLIP a √©t√© entra√Æn√© principalement sur de l'anglais

3. **Le multilinguisme am√©liore certaines cat√©gories**
   - Gains spectaculaires (+70%) sur des classes sp√©cifiques
   - Chaque langue capture des aspects compl√©mentaires


## Visualisations

Les scripts g√©n√®rent des graphiques PNG avec :
- Comparaison des performances par langue
- Corr√©lations cross-linguales
- Distribution par classe
- Top gainers/losers du multilinguisme

## R√©f√©rences

- **CLIP Paper** : Radford et al. (2021) - "Learning Transferable Visual Models From Natural Language Supervision"
- **Dataset** : CIFAR-100 (Krizhevsky, 2009)


---

**Auteur** : Marine VIEILLARD / Projet M2 Maths & IA - NLP  
**Date** : D√©cembre 2024